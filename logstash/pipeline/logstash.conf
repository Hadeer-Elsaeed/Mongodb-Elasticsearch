
input {
  mongodb {
    uri => "mongodb://root:123456@mongodb:27017/logstash?authSource=admin"
    placeholder_db_dir => '/opt/logstash/'
    placeholder_db_name => 'logstash_sqlite.db'
    collection => "db_collection"
    batch_size => 10000
    
  }
}

filter {
    ruby {
        code => '
        event.set("log_entry", event.get("log_entry").gsub(/=>/, ":").gsub(/BSON::ObjectId\([^)]+\)/, "\"[ObjectId]\""))
        event.set("mongodb_data", JSON.parse(event.get("log_entry")))
        keys = ["name", "address"]
        for key_to_extract in keys
            event.set(key_to_extract, event.get("mongodb_data")[key_to_extract])
        event.set("client_id", event.get("mongodb_data")["name"][0]["client_id"])
        end
        '
    }
    mutate {
        rename => {"_id" => "id"}
        gsub => [ "message", "null", "None" ]

    }

    prune {
        blacklist_names => ["log_entry", "mongodb_data"]
    }
}


output {
    elasticsearch {
        hosts => ["http://elasticsearch:9200"]
        user => "elastic"
        password => "root"
        index => "demo_index"
        document_id => "%{client_id}"  # Use the client_id field as the document ID
        action => "update"  # Use the "update" action
        doc_as_upsert => true  # Create a new document if it doesn't exist
    }
}
